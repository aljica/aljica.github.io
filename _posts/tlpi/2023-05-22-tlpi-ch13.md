---
layout: post
title: The Linux Programming Interface Chapter 13 File I/O Buffering Summary
categories: tlpi
---

# 13.1 FILE I/O BUFFERING

	• When write()'ing to a file, data is taken from user-space and placed in a kernel buffer cache.
		○ From the kernel buffer cache, data is later on written to the file on disk.
		○ In the meantime, if a process tries to access the new data, the kernel buffer supplies it (and the outdated contents of the on-disk file is not supplied)
	• When read()'ing from a file, data is once again placed into a kernel buffer cache.
		○ Other processes read from this kernel buffer cache until it is "exhausted".
	• This makes data access more efficient.
		○ Also, when the kernel flushes data, it can flush more data at once, reducing the number of disk operations required.
	• There is no memory limit; the kernel will use as much as required to keep the data.
		○ Physical memory is an obvious limitation.
		○ If system runs low on memory, some will be written to disk/swapped out of course.
	• In practice, newer implementations of Linux don't keep a separate buffer cache.
		○ The buffer cache is kept in the regular page cache.
			§ Regular page cache also includes memory-mapped files.
	• Write 1 byte 1000 times or 1000 bytes once => SAME number of disk operations!!
		○ However, 1000 vs 1 syscall.
		○ Syscalls carry a lot of overhead; trapping, checking system arguments, copying data from user-space to kernel-space - all these context switches cost a lot.
		○ So, 1000 bytes once is preferred.
	• BUF_SIZE can be used to set various buffer sizes, e.g. from 1 to 65536 in this example (testing performance).
		○ There is a pretty significant difference between the lowest and highest tested, in terms of total CPU time (both user-space CPU time and kernel-space CPU time).
		○ The greater the BUF_SIZE, the faster the disk read/write.
		○ If you set BUF_SIZE to 4096, you'll get almost optimal performance increases. Higher BUF_SIZES don't do much; the cost of copying data from user to kernel space carries most of the overhead (and to perform disk I/O).
		○ Conclusion: Disk reads are actually the most expensive. But system calls also carry overhead.

# 13.2 Buffering in the stdio library

	• Functions that do buffering to avoid making many system calls.
	• Setvbuf(file stream, buf, mode, size_of_buf)
		○ Buf must be statically/dynamically allocated on the heap. Chaos will ensue if it's on the stack, once that stack frame returns.
		○ If buf is NULL, glibc implementation creates the buf for you.
		○ Flags
			§ _IONBF
				□ Don't buffer I/O at all.
			§ _IOLBF
				□ Fully buffer using the full buffer size.
			§ _IOFBF
				□ Line-buffered I/O.
				□ Read/write a line at a time; e.g. in terminal devices.
	• Setbuf(), setbuffer(), etc…
	• Fflush()
		○ OUTPUT Data in buffer is flushed to kernel buffer via write().
		○ Applying fflush() to an INPUT stream DISCARDS the inputted data.
			§ Frequently used in terminal programs where e.g. printf("Hi world"); is used without a \n character.
				□ This flushes input and avoids printing the cmdline prompt again.

# 13.3 Controlling kernel buffering of I/O

	• Imagine a database journalling process (database backup)
		○ It may want to force flushing the kernel buffer (for an output file) to a hard disk drive before continuing.
		○ This can be controlled via certain syscalls.
	• SUSv3 - Synchronized I/O data integrity completion
		○ Synchronizing here means:
			§ When write()'ing to a hard disk, you are not only transferring the contents of the file, but also the metadata about the file; especially the parts of metadata that are vital for the future retrieval of that file.
				□ E.g. timestamp may not be vital for future retrieval; but file size may be!
	• Synchronized I/O file integrity completion
		○ Sync I/O data integrity completion is a SUBSET of this
		○ File integrity means that ALL metadata is transferred - Sync I/O data integrity means only the VITAL parts of metadata (for future retrieval) is transferred to disk.
	• Syscalls:
		○ Fsync(int fd)
			§ File integrity synchronization
			§ Takes more disk operations to write both the updated metadata & the actual updated file
			§ Returns once file and metadata have been written to the disk from the disk cache or simply written to disk directly from the kernel buffer
		○ Fdatasync(int fd)
			§ Data integrity sync
			§ Fewer disk ops
			§ For example, if the contents of a file are modified, then metadata about the file does not have to change to ensure future retrieval. So e.g. timestamp does not really have to be updated in the metadata file. This would thus return as soon as the file on disk has the data modified, and timestamps etc in the metadata file will be updated asynchronously.
	• Controlling kenrel buffering and flushing of I/O can lead to significant performance gains
		○ On disk, data and metadata tends to be at different locations
		○ Lots of performance is won if you reduce the need to write to both areas synchronously
	• Void sync()
		○ Flushes all kernel buffers to disk
		○ Can return asynchronously, i.e. once it's done
		○ Permanently running threads run sync() every time period (e.g. 30 seconds) to ensure data integrity between data on disk and in kernel buffer.
	• O_SYNC performance degradation
		○ Fd = open(…, O_SYNC)
			§ Using the O_SYNC flag when opening a file will slow things down tremendously.
				□ Same thing with using fsync() or fdatasync() frequently.
			§ The reason is: now, both file contents and metadata about the file has to be written to disk before file writes to that fd return.
			§ It is even worse if you turn off hard disk caching; because the O_SYNC returns as long as it transfers file contents and metadata at least to the HDD cache.
			§ BUF_SIZE is once again a big factor in how long it takes.
				□ 1 BYTE BUF_SIZE is tremendously slow; 4096 byte buf size is much faster, but still comparatively very slow if we did not use O_SYNC at all.
	• O_DSYNC and O_RSYNC
		○ O_DSYNC is the DATA INTEGRITY SYNC version of O_SYNC (O_SYNC is FILE INTEGRITY SYNC)
		○ O_RSYNC is specified in conjunction with either O_DSYNC and O_SYNC
			§ In the case of O_RSYNC being specified with O_DSYNC
				□ Because of O_RSYNC, if specified with O_DSYNC, a file read would wait for file writes relevant to DATA INTEGRITY (i.e. actual file content writes to disk) to complete before reading
				□ Because of O_RSYNC, if specified with O_SYNC, a file read would wait for all file writes to complete before reading (including metadata file writes).

# Summary of I/O buffering

![Image](/docs/assets/images/ch-13/tlpi-ch13-1.jpg)

	• Stdio library has its own buffer in user-space. 
	• Setbuf(file_stream, NULL)
		○ This would set the stdio buffer size to NULL and mean auto-flushing occurs to the kernel buffer cache
	• On the kernel buffer cache level, fsync(), fdatasync(), open with O_SYNC flag etc become relevant for when disk reads/writes happen.

### Advising the kernel about I/O patterns

	• Posix_fadvise(int fd, offset, len, advice)
		○ Advising the kernel on how the file will be accessed, for optimization purposes
		○ First, you specify the file d, the offset in that file and the len
		○ The advice can be: it's a flag like POSIX_FADV_NORMAL, etc.
		○ Each flag represents something: one might be that this region will only be accessed once, or that it will be accessed more times, etc.
		○ Kernel can use this information to optimize usage of the buffer cache; but it doesn't have to use the information.

### Bypassing the buffer cache: Direct I/O

	• Direct I/O means skipping the kernel buffer cache and writing directly to disk.
	• Problem: you skip the kernel's optimizations, such as: sequential read-ahead, writing to disk in blocks, and processes sharing the kernel buffer.
	• Use case: For a DBMS that implements its own optimizations.
	• May not be implemented on some file systems like VFAT, (but most UNIX file system implementations do support it).
	• AVOID having one process open a file with O_DIRECT and another without.
		○ NO coherency between kernel buffer cache and the direct disk writing. AVOID!
	• There are some alignment restrictions for direct I/O
		○ Block size of the disk drive: usually 512 bytes. (Might also be, for some Linux implementations, that the restriction is the underlying file system logical block size; 1024, 2048 or 4096 bytes most commonly.
		○ Basically, when you begin data transfer, the offset into the disk or file residing on disk must be a multiple of 512 bytes (the block size, whichever value it has).
		○ Length of data also has to have this block size.
		○ Must also be aligned on a memory boundary that is a multiple of this block size.

### Mixing Library functions and system calls for file I/O

	• Int fileno(FILEstream)
		○ Turns a streamed file into a file descriptor so you can use read(), write() and other syscalls on it
	• FILE fdopen(int fd, mode)
		○ Turns an fd into a stream file.
		○ Specify mode, e.g. read, write, append, etc.
	• E.g. sockets/pipes => regular files, and you need to use fdopen() to turn them into stream files!!

	• IMPORTANT!
	• Be careful of mixing:
		○ The printf() is an stdio library call which has data stored in the user space buffer, which you can flush to the kernel buffer cache using fflush().
		○ However, before that happens, write() has time to add some contents to the standard output file descriptor, namely the string "stop playing\n".
		○ Then, the printf() eventually gets flushed and transferred to the kernel buffer cache.
		○ Then, the kernel buffer cache is displayed to the screen.
		○ So basically you're "polluting" the kernel buffer cache with the write().
		○ IF we had used a \n in the printf() function, because stdout is line-buffered when connected to a terminal, the user space buffer would be flushed (or if we used fflush() after the printf()).

![Image](/docs/assets/images/ch-13/tlpi-ch13-3.png)

![Image](/docs/assets/images/ch-13/tlpi-ch13-4.png)

![Image](/docs/assets/images/ch-13/tlpi-ch13-5.png)

![Image](/docs/assets/images/ch-13/tlpi-ch13-2.png)